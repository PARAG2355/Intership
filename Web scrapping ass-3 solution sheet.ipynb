{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75ac1c0",
   "metadata": {},
   "source": [
    "# WEB SCRAPING-ASSIGNMENT3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fa679",
   "metadata": {},
   "source": [
    "### Solution sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905358e",
   "metadata": {},
   "source": [
    "1.Write a python program which searches all the product under a particular product from www.amazon.in. The \n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for \n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product_name, num_pages=1, base_url=\"https://www.amazon.in/s?k={}\"):\n",
    "    \"\"\"\n",
    "    Searches for products under a given product name on Amazon.in and extracts basic details.\n",
    "\n",
    "    Args:\n",
    "        product_name (str): The name of the product to search for.\n",
    "        num_pages (int, optional): The number of pages to search (default: 1).\n",
    "        base_url (str, optional): The base URL for search queries (default: Amazon India search).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains extracted product details.\n",
    "\n",
    "    Raises:\n",
    "        HTTPError: If an HTTP error occurs during requests.\n",
    "        ValueError: If product_name is empty or num_pages is negative.\n",
    "    \"\"\"\n",
    "\n",
    "    if not product_name:\n",
    "        raise ValueError(\"product_name cannot be empty\")\n",
    "    if num_pages < 1:\n",
    "        raise ValueError(\"num_pages must be at least 1\")\n",
    "\n",
    "    products = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url.format(product_name.replace(\" \", \"+\") + f\"&page={page}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "        # Refine selectors based on Amazon.in's dynamic layout\n",
    "        product_elements = soup.find_all(\n",
    "            lambda tag: tag.name == \"div\" and tag.has_attr(\"data-component-type\") and \"s-search-result\" in tag[\"data-component-type\"]\n",
    "        )\n",
    "\n",
    "        for product_element in product_elements:\n",
    "            try:\n",
    "                # Extract product details using refined selectors\n",
    "                product_url = product_element.find(\"a\", href=True)[\"href\"]\n",
    "                product_name = product_element.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\").text.strip()\n",
    "                price_element = product_element.find(\"span\", class_=\"a-price-whole\")\n",
    "                price = price_element.text.strip() if price_element else None\n",
    "                rating_element = product_element.find(\"span\", class_=\"a-star-normal\")\n",
    "                rating = rating_element.get(\"aria-label\") if rating_element else None\n",
    "\n",
    "                products.append(\n",
    "                    {\n",
    "                        \"url\": product_url,\n",
    "                        \"name\": product_name,\n",
    "                        \"price\": price,\n",
    "                        \"rating\": rating,\n",
    "                    }\n",
    "                )\n",
    "            except AttributeError:  # Ignore invalid product elements\n",
    "                pass\n",
    "\n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter a product name to search for: \")\n",
    "    num_pages = int(input(\"Enter the number of pages to search (default: 1): \") or 1)\n",
    "    products = search_amazon_products(product_name, num_pages)\n",
    "\n",
    "    if products:\n",
    "        for product in products:\n",
    "            print(f\"Product Name: {product['name']}\")\n",
    "            print(f\"Product URL: {product['url']}\")\n",
    "            if product[\"price\"]:\n",
    "                print(f\"Price: ₹{product['price']}\")\n",
    "            if product[\"rating\"]:\n",
    "                print(f\"Rating: {product['rating']}\")\n",
    "            print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"No products found for the given search query.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046da844",
   "metadata": {},
   "source": [
    "2.In the above question, now scrape the following details of each product listed in first 3 pages of your search \n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then \n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fe95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def search_amazon_products(product_name, num_pages=3, base_url=\"https://www.amazon.in/\"):\n",
    "    products_data = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = f\"{base_url}s?k={product_name}&page={page}\"\n",
    "\n",
    "        # Make a request to the Amazon search results page\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract product details from the page\n",
    "        products = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        for product in products:\n",
    "            brand_name = product.find('span', class_='a-size-base-plus a-color-base').text.strip()\n",
    "            product_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal').text.strip()\n",
    "\n",
    "            # Extracting other details, replace with '-' if not found\n",
    "            price = product.find('span', class_='a-offscreen')\n",
    "            price = price.text.strip() if price else '-'\n",
    "\n",
    "            return_exchange = product.find('div', class_='a-row a-size-small')\n",
    "            return_exchange = return_exchange.text.strip() if return_exchange else '-'\n",
    "\n",
    "            expected_delivery = product.find('span', class_='a-text-bold')\n",
    "            expected_delivery = expected_delivery.text.strip() if expected_delivery else '-'\n",
    "\n",
    "            availability = product.find('span', class_='a-size-medium a-color-success')\n",
    "            availability = availability.text.strip() if availability else '-'\n",
    "\n",
    "            product_url = product.find('a', class_='a-link-normal')['href']\n",
    "            product_url = f\"{base_url}{product_url}\" if product_url.startswith('/') else product_url\n",
    "\n",
    "            # Append product details to the list\n",
    "            products_data.append({\n",
    "                'Brand Name': brand_name,\n",
    "                'Name of the Product': product_name,\n",
    "                'Price': price,\n",
    "                'Return/Exchange': return_exchange,\n",
    "                'Expected Delivery': expected_delivery,\n",
    "                'Availability': availability,\n",
    "                'Product URL': product_url\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list of product details\n",
    "    df = pd.DataFrame(products_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('amazon_products_data.csv', index=False)\n",
    "\n",
    "# Example usage\n",
    "search_amazon_products(\"laptop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d05b52",
   "metadata": {},
   "source": [
    "3.Write a python program to access the search bar and search button on images.google.com and scrape 10 \n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6366d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def google_images_scraper(keywords, num_images=10):\n",
    "    # Set up the WebDriver (specify the path to your WebDriver)\n",
    "    driver_path = '/path/to/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Open Google Images\n",
    "        driver.get(\"https://images.google.com/\")\n",
    "\n",
    "        # Find the search bar and enter the keyword\n",
    "        search_bar = driver.find_element(\"name\", \"q\")\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(keyword)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for the search results to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll down to load more images\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Find and scrape image URLs\n",
    "        img_elements = driver.find_elements_by_css_selector('img.Q4LuWd')\n",
    "        image_urls = [img.get_attribute('src') for img in img_elements]\n",
    "\n",
    "        # Download the images\n",
    "        save_directory = f\"{keyword}_images\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        for i, img_url in enumerate(image_urls[:num_images]):\n",
    "            response = requests.get(img_url, stream=True)\n",
    "            with open(f\"{save_directory}/image_{i + 1}.jpg\", 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "# Keywords for image searches\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Specify the number of images to scrape for each keyword\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "# Run the scraper\n",
    "google_images_scraper(keywords, num_images_per_keyword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e05a35",
   "metadata": {},
   "source": [
    "4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(keyword):\n",
    "    url = f'https://www.flipkart.com/search?q={keyword}&page=1'\n",
    "\n",
    "    # Make a request to the Flipkart search results page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract smartphone details from the page\n",
    "    products = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "    smartphones_data = []\n",
    "\n",
    "    for product in products:\n",
    "        brand_name = product.find('div', class_='_4rR01T').text.strip()\n",
    "        smartphone_name = product.find('a', class_='IRpwTa').text.strip()\n",
    "        colour = product.find('div', class_='uEhfTl').text.strip()\n",
    "\n",
    "        details = product.find_all('li', class_='rgWa7D')\n",
    "        ram = details[0].text.strip() if len(details) > 0 else '-'\n",
    "        storage = details[1].text.strip() if len(details) > 1 else '-'\n",
    "        primary_camera = details[2].text.strip() if len(details) > 2 else '-'\n",
    "        secondary_camera = details[3].text.strip() if len(details) > 3 else '-'\n",
    "        display_size = details[4].text.strip() if len(details) > 4 else '-'\n",
    "        battery_capacity = details[5].text.strip() if len(details) > 5 else '-'\n",
    "\n",
    "        price = product.find('div', class_='_30jeq3').text.strip()\n",
    "\n",
    "        product_url = product.find('a', class_='IRpwTa')['href']\n",
    "        product_url = f\"https://www.flipkart.com{product_url}\" if product_url.startswith('/') else product_url\n",
    "\n",
    "        # Append smartphone details to the list\n",
    "        smartphones_data.append({\n",
    "            'Brand Name': brand_name,\n",
    "            'Smartphone Name': smartphone_name,\n",
    "            'Colour': colour,\n",
    "            'RAM': ram,\n",
    "            'Storage(ROM)': storage,\n",
    "            'Primary Camera': primary_camera,\n",
    "            'Secondary Camera': secondary_camera,\n",
    "            'Display Size': display_size,\n",
    "            'Battery Capacity': battery_capacity,\n",
    "            'Price': price,\n",
    "            'Product URL': product_url\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list of smartphone details\n",
    "    df = pd.DataFrame(smartphones_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f'{keyword}_smartphones_data.csv', index=False)\n",
    "\n",
    "# Example usage\n",
    "search_keyword = 'Oneplus Nord'  # You can replace this with any smartphone you want to search\n",
    "scrape_flipkart_smartphones(search_keyword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6aa5c",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(api_key, city_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "\n",
    "    # Prepare parameters for the API request\n",
    "    params = {\n",
    "        'address': city_name,\n",
    "        'key': api_key,\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if data['status'] == 'OK':\n",
    "        # Extract latitude and longitude\n",
    "        location = data['results'][0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(f\"Error: {data['status']}\")\n",
    "        return None\n",
    "\n",
    "# Replace 'your_api_key' with your actual Google Maps API key\n",
    "google_maps_api_key = 'your_api_key'\n",
    "city_to_search = 'New York'  # Replace with the city you want to search\n",
    "\n",
    "coordinates = get_coordinates(google_maps_api_key, city_to_search)\n",
    "\n",
    "if coordinates:\n",
    "    print(f\"Coordinates for {city_to_search}: Latitude = {coordinates[0]}, Longitude = {coordinates[1]}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve coordinates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81ef0b",
   "metadata": {},
   "source": [
    "6.Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    # Make a request to the digit.in page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract gaming laptop details from the page\n",
    "    laptops_data = []\n",
    "\n",
    "    products = soup.find_all('div', class_='TopNumbeHeading')\n",
    "\n",
    "    for product in products:\n",
    "        laptop_name = product.find('div', class_='TopNumbeProductTitle').text.strip()\n",
    "        specs = product.find('div', class_='TopNumbeRow').text.strip().split('|')\n",
    "        \n",
    "        processor = specs[0].strip() if len(specs) > 0 else '-'\n",
    "        ram = specs[1].strip() if len(specs) > 1 else '-'\n",
    "        storage = specs[2].strip() if len(specs) > 2 else '-'\n",
    "        display_size = specs[3].strip() if len(specs) > 3 else '-'\n",
    "        price = product.find('div', class_='TopNumbePrice').text.strip()\n",
    "\n",
    "        # Append laptop details to the list\n",
    "        laptops_data.append({\n",
    "            'Laptop Name': laptop_name,\n",
    "            'Processor': processor,\n",
    "            'RAM': ram,\n",
    "            'Storage': storage,\n",
    "            'Display Size': display_size,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list of laptop details\n",
    "    df = pd.DataFrame(laptops_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('digit_gaming_laptops_data.csv', index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_digit_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cace8",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ec14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Make a request to the Forbes Billionaires page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract billionaire details from the page\n",
    "    billionaires_data = []\n",
    "\n",
    "    billionaires = soup.find_all('div', class_='personName')\n",
    "\n",
    "    for billionaire in billionaires:\n",
    "        rank = billionaire.find_previous('div', class_='rank').text.strip()\n",
    "        name = billionaire.text.strip()\n",
    "\n",
    "        # Find the parent div and extract other details\n",
    "        parent_div = billionaire.find_parent('div', class_='personInfo')\n",
    "        net_worth = parent_div.find('div', class_='netWorth').text.strip()\n",
    "        age = parent_div.find('div', class_='age').text.strip()\n",
    "        citizenship = parent_div.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        source = parent_div.find('div', class_='source').text.strip()\n",
    "        industry = parent_div.find('div', class_='category').text.strip()\n",
    "\n",
    "        # Append billionaire details to the list\n",
    "        billionaires_data.append({\n",
    "            'Rank': rank,\n",
    "            'Name': name,\n",
    "            'Net Worth': net_worth,\n",
    "            'Age': age,\n",
    "            'Citizenship': citizenship,\n",
    "            'Source': source,\n",
    "            'Industry': industry\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list of billionaire details\n",
    "    df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('forbes_billionaires_data.csv', index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ded0cd",
   "metadata": {},
   "source": [
    "8.Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "\n",
    "def get_youtube_comments(api_key, video_id, max_comments=500):\n",
    "    # Set up the YouTube Data API\n",
    "    youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get video comments\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=max_comments\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    comments_data = []\n",
    "\n",
    "    # Extract comments, comment upvotes, and time when comments were posted\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        upvotes = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        time_posted = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "\n",
    "        comments_data.append({\n",
    "            'Comment': comment,\n",
    "            'Upvotes': upvotes,\n",
    "            'Time Posted': time_posted\n",
    "        })\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# Replace 'your_api_key' and 'your_video_id' with your actual API key and YouTube video ID\n",
    "api_key = 'your_api_key'\n",
    "video_id = 'your_video_id'\n",
    "\n",
    "comments_data = get_youtube_comments(api_key, video_id)\n",
    "\n",
    "# Print the first few comments\n",
    "for i, comment_info in enumerate(comments_data[:5], start=1):\n",
    "    print(f\"Comment {i}:\")\n",
    "    print(f\"Comment: {comment_info['Comment']}\")\n",
    "    print(f\"Upvotes: {comment_info['Upvotes']}\")\n",
    "    print(f\"Time Posted: {comment_info['Time Posted']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c926b0",
   "metadata": {},
   "source": [
    "9.Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall \n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostelworld_data(location='London'):\n",
    "    url = f'https://www.hostelworld.com/s?q={location}&country=England&city={location}&dateFrom=2024-02-01&dateTo=2024-02-07&number_of_guests=1&page=1'\n",
    "\n",
    "    # Make a request to the Hostelworld page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract hostel details from the page\n",
    "    hostels_data = []\n",
    "\n",
    "    hostels = soup.find_all('div', class_='property-card')\n",
    "\n",
    "    for hostel in hostels:\n",
    "        name = hostel.find('h2', class_='title title-6').text.strip()\n",
    "        distance = hostel.find('span', class_='description').text.strip().split()[0]\n",
    "        ratings = hostel.find('div', class_='score orange big').text.strip()\n",
    "        total_reviews = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "        overall_reviews = hostel.find('div', class_='reviews').text.strip().split()[3]\n",
    "        privates_price = hostel.find('div', class_='price-col privates from-price').text.strip()\n",
    "        dorms_price = hostel.find('div', class_='price-col dorms from-price').text.strip()\n",
    "        facilities = ', '.join([facility.text.strip() for facility in hostel.find_all('li', class_='facility-badge')])\n",
    "        property_description = hostel.find('div', class_='rating-factors').find_next('span').text.strip()\n",
    "\n",
    "        # Append hostel details to the list\n",
    "        hostels_data.append({\n",
    "            'Name': name,\n",
    "            'Distance from City Centre': distance,\n",
    "            'Ratings': ratings,\n",
    "            'Total Reviews': total_reviews,\n",
    "            'Overall Reviews': overall_reviews,\n",
    "            'Privates from Price': privates_price,\n",
    "            'Dorms from Price': dorms_price,\n",
    "            'Facilities': facilities,\n",
    "            'Property Description': property_description\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list of hostel details\n",
    "    df = pd.DataFrame(hostels_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f'{location}_hostels_data.csv', index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_hostelworld_data()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70c05f77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
